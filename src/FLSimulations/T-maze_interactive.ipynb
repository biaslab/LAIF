{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Maze Interactive Simulation\n",
    "\n",
    "This notebook generates the expected reward landscapes of Fig. 10 (Sec. 7.2) for the CBFE and EFE agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using ForneyLab\n",
    "using Plots\n",
    "using ProgressMeter\n",
    "\n",
    "# T-maze layout\n",
    "# [2| |3]\n",
    "#   | |\n",
    "#   |1|\n",
    "#   |4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2\n",
    "\n",
    "fg_plan = FactorGraph()\n",
    "\n",
    "u = Vector{Variable}(undef, T)\n",
    "x = Vector{Variable}(undef, T)\n",
    "y = Vector{Variable}(undef, T)\n",
    "\n",
    "@RV x_t_min ~ Categorical(placeholder(:D_t_min, dims=(8,)))\n",
    "\n",
    "x_k_min = x_t_min\n",
    "for k=1:T\n",
    "    @RV u[k]\n",
    "    @RV x[k] ~ Transition(x_k_min, u[k])\n",
    "    @RV y[k] ~ Transition(x[k], placeholder(:A, dims=(16,8), var_id=:A_*k))\n",
    "\n",
    "    placeholder(u[k], :u, index=k, dims=(8,8))\n",
    "    Categorical(y[k], placeholder(:C, dims=(16,), index=k, var_id=:C_*k))\n",
    "    PointMassConstraint(y[k])\n",
    "    \n",
    "    x_k_min = x[k]\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_plan_constrained = PosteriorFactorization(y, [x_t_min; x], ids=[:Y, :X])\n",
    "algo_plan_constrained = messagePassingAlgorithm(y, id=:PlanConstrained, free_energy=true)\n",
    "code_plan_constrained = algorithmSourceCode(algo_plan_constrained, free_energy=true)\n",
    "eval(Meta.parse(code_plan_constrained))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slide Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_slide = FactorGraph()\n",
    "\n",
    "u = Vector{Variable}(undef, T)\n",
    "x = Vector{Variable}(undef, T)\n",
    "y = Vector{Variable}(undef, T)\n",
    "\n",
    "@RV x_t_min ~ Categorical(placeholder(:D_t_min, dims=(8,)))\n",
    "@RV x_t ~ Transition(x_t_min, placeholder(:B_t, dims=(8,8)))\n",
    "@RV y_t ~ Transition(x_t, placeholder(:A, dims=(16,8)))\n",
    "placeholder(y_t, :o_t, dims=(16,))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_slide = PosteriorFactorization(fg_slide)\n",
    "algo_slide = messagePassingAlgorithm(x_t, id=:Slide)\n",
    "code_slide = algorithmSourceCode(algo_slide)\n",
    "eval(Meta.parse(code_slide))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# println(code_slide) # Uncomment to inspect generated source code for slide step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 # Number of moves per simulation\n",
    "alphas = 0.5:0.025:1.0\n",
    "# alphas = collect(0.5:0.05:0.999)\n",
    "# push!(alphas, 0.99) # Avoid Î±=1.0 for stability reasons\n",
    "cs = 0.0:0.1:2.0\n",
    "J = length(alphas)\n",
    "K = length(cs)\n",
    "S = 10 # Number of simulations\n",
    "\n",
    "include(\"environment.jl\")\n",
    "include(\"agent.jl\")\n",
    "include(\"update_rules.jl\")\n",
    "include(\"helpers.jl\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_con = Matrix{Vector{Float64}}(undef, J, K)\n",
    "P_con = Matrix{Vector{Vector{Int64}}}(undef, J, K)\n",
    "@showprogress for j=1:J\n",
    "    for k=1:K\n",
    "        (A, B, C, D) = constructABCD(alphas[j], cs[k])\n",
    "        \n",
    "        R_con[j,k] = []\n",
    "        P_con[j,k] = []\n",
    "        for s = 1:S\n",
    "            (execute, observe) = initializeWorld(A, B, C, D) # Let there be a world\n",
    "            (plan, _, act, slide) = initializeAgent(A, B, C, D) # Let there be a constrained agent\n",
    "\n",
    "            # Step through the experimental protocol\n",
    "            r_t = 0.0 # Reward\n",
    "            a = Vector{Int64}(undef, N)\n",
    "            for t = 1:N\n",
    "                       F_t = plan()\n",
    "                      a[t] = act(F_t)\n",
    "                             execute(a[t])\n",
    "                (o_t, r_t) = observe()\n",
    "                             slide(a[t], o_t)\n",
    "            end\n",
    "\n",
    "            push!(R_con[j,k], r_t) # Reward is collected after second move\n",
    "            push!(P_con[j,k], a) # Store executed actions\n",
    "        end\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plotReward(alphas, cs, R_con, dpi=300)\n",
    "# annotateActions(p, alphas, cs, P_con) # Uncomment to annotate figure with performed actions\n",
    "savefig(\"figures/constrained_reward.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_efe = Matrix{Vector{Float64}}(undef, J, K)\n",
    "P_efe = Matrix{Vector{Vector{Int64}}}(undef, J, K)\n",
    "@showprogress for j=1:J\n",
    "    for k=1:K\n",
    "        (A, B, C, D) = constructABCD(alphas[j], cs[k])\n",
    "\n",
    "        R_efe[j,k] = []\n",
    "        P_efe[j,k] = []\n",
    "        for s = 1:S\n",
    "            (execute, observe) = initializeWorld(A, B, C, D) # Let there be a world\n",
    "            (_, plan, act, slide) = initializeAgent(A, B, C, D) # Let there be an EFE agent\n",
    "\n",
    "            # Step through the experimental protocol\n",
    "            r_t = 0.0 # Reward\n",
    "            a = Vector{Int64}(undef, N)\n",
    "            for t = 1:N\n",
    "                       G_t = plan()\n",
    "                      a[t] = act(G_t)\n",
    "                             execute(a[t])\n",
    "                (o_t, r_t) = observe()\n",
    "                             slide(a[t], o_t)\n",
    "            end\n",
    "\n",
    "            push!(R_efe[j,k], r_t) # Reward is collected after second move\n",
    "            push!(P_efe[j,k], a) # Store actions\n",
    "        end\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = plotReward(alphas, cs, R_efe)\n",
    "# annotateActions(p, alphas, cs, P_efe) # Uncomment to annotate figure with performed actions\n",
    "savefig(\"figures/efe_reward.png\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
