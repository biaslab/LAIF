{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Maze Multi-Agent Bargaining Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RxInfer, LinearAlgebra, Plots\n",
    "\n",
    "include(\"helpers.jl\")\n",
    "include(\"../goal_observation.jl\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "αs = [0.8, 0.85, 0.9, 0.95, 1.0] # Possible offers\n",
    "L = length(αs)\n",
    "c = 2.0\n",
    "S = 30\n",
    "seed = 666\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function t_maze_primary(A, D, x)\n",
    "    u = datavar(Matrix{Int64}, 2) # Policy for evaluations\n",
    "    z = randomvar(2) # Latent states\n",
    "    c = datavar(Vector{Float64}, 2) # Goal prior statistics\n",
    "\n",
    "    z_0 ~ Categorical(D) # State prior\n",
    "\n",
    "    z_k_min = z_0\n",
    "    for k=1:2\n",
    "        z[k] ~ Transition(z_k_min, u[k])\n",
    "        c[k] ~ GoalObservation(z[k], A) where { # Observation matrix depends on offer by secondary agent\n",
    "            meta=GeneralizedMeta(x[k]), \n",
    "            pipeline=GeneralizedPipeline(vague(Categorical,16))}\n",
    "\n",
    "        z_k_min = z[k] # Reset for next slice\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"primary_agent.jl\")\n",
    "include(\"primary_environment.jl\") # Environment for primary agent\n",
    "\n",
    "(B, C, D) = constructPrimaryBCD(c)\n",
    "\n",
    "rs = generateGoalSequence(seed, S) # Sets random seed and returns reproducible goal sequence\n",
    "(reset, execute, observe) = initializePrimaryWorld(B, rs) # Define interation (Markov blanket) with the T-maze environment\n",
    "(infer, act) = initializePrimaryAgent(B, C, D)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables in the secondary agent are indicated by \"prime\"\n",
    "@model function t_maze_secondary(A_prime_s, x_prime, alpha_s)\n",
    "    c_prime = datavar(Vector{Float64})\n",
    "\n",
    "    A_prime ~ MatrixDirichlet(A_prime_s)\n",
    "    c_prime ~ GoalObservation(alpha_s, A_prime) where {\n",
    "                meta=GeneralizedMeta(x_prime),\n",
    "                pipeline=GeneralizedPipeline()}\n",
    "end\n",
    "\n",
    "@constraints function structured(approximate::Bool)\n",
    "    if approximate\n",
    "        q(A_prime) :: SampleList(20)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"secondary_agent.jl\")\n",
    "include(\"secondary_environment.jl\") # Environment for secondary agent represents an interaction with the primary agent\n",
    "\n",
    "A_prime_0 = constructSecondaryPriors()\n",
    "\n",
    "(execute_prime, observe_prime) = initializeSecondaryWorld() # Defines interaction (Markov blanket) with primary agent\n",
    "(infer_prime, act_prime) = initializeSecondaryAgent(A_prime_0)\n",
    "\n",
    "# Step through the experimental protocol\n",
    "A_primes = Vector{Matrix}(undef, S) # Posterior statistics for A_p\n",
    "G_primes = Vector{Vector}(undef, S) # Free energy values\n",
    "a_primes = Vector{Union{Int64, Missing}}(missing, S) # Actions per time\n",
    "o_primes = Vector{Union{Vector, Missing}}(missing, S) # Observations (one-hot) per time\n",
    "for s = 1:S\n",
    "    # Make offer at t=1\n",
    "    (G_primes[s], _) = infer_prime(1, a_primes[s], o_primes[s])\n",
    "         a_primes[s] = act_prime(G_primes[s])\n",
    "                       execute_prime(s, a_primes[s]) # Triggers inference in primary agent\n",
    "         o_primes[s] = observe_prime() # Observes cue-visit of primary agent\n",
    "    \n",
    "    # Learn at t=2        \n",
    "    (_, A_primes[s]) = infer_prime(2, a_primes[s], o_primes[s])\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"visualizations.jl\")\n",
    "plotOffers(G_primes, a_primes, o_primes)\n",
    "savefig(\"figures/GFE_offers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
